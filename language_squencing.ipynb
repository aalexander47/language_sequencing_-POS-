{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three sentences\n",
      "[OneWord(word='CRICKET', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='-', pos_label=':', chunk_label='O', entity_label='O'), OneWord(word='LEICESTERSHIRE', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='TAKE', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='OVER', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='AT', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='TOP', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='AFTER', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='INNINGS', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='VICTORY', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='.', pos_label='.', chunk_label='O', entity_label='O')]\n",
      "\n",
      "[OneWord(word='LONDON', pos_label='NNP', chunk_label='B-NP', entity_label='B-LOC'), OneWord(word='1996-08-30', pos_label='CD', chunk_label='I-NP', entity_label='O')]\n",
      "\n",
      "[OneWord(word='West', pos_label='NNP', chunk_label='B-NP', entity_label='B-MISC'), OneWord(word='Indian', pos_label='NNP', chunk_label='I-NP', entity_label='I-MISC'), OneWord(word='all-rounder', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='Phil', pos_label='NNP', chunk_label='I-NP', entity_label='B-PER'), OneWord(word='Simmons', pos_label='NNP', chunk_label='I-NP', entity_label='I-PER'), OneWord(word='took', pos_label='VBD', chunk_label='B-VP', entity_label='O'), OneWord(word='four', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='for', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='38', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='on', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='Friday', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='as', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='Leicestershire', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='beat', pos_label='VBD', chunk_label='B-VP', entity_label='O'), OneWord(word='Somerset', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='by', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='an', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='innings', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='and', pos_label='CC', chunk_label='O', entity_label='O'), OneWord(word='39', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='runs', pos_label='NNS', chunk_label='I-NP', entity_label='O'), OneWord(word='in', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='two', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='days', pos_label='NNS', chunk_label='I-NP', entity_label='O'), OneWord(word='to', pos_label='TO', chunk_label='B-VP', entity_label='O'), OneWord(word='take', pos_label='VB', chunk_label='I-VP', entity_label='O'), OneWord(word='over', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='at', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='the', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='head', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='of', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='the', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='county', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='championship', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='.', pos_label='.', chunk_label='O', entity_label='O')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "#Same as tuple but the fields are named for convenience\n",
    "#this says we have four fields\n",
    "OneWord=namedtuple(\"OneWord\",[\"word\",\"pos_label\",\"chunk_label\",\"entity_label\"])\n",
    "\n",
    "def read_conll2003(f_name):\n",
    "    \"\"\"Yield complete sentences\"\"\"\n",
    "    current_sentence=[] #This will be a list of (word,label), which we accumulate for each sentence\n",
    "    with open(f_name) as f:\n",
    "        for line in f:\n",
    "            line=line.strip() #drop whitespace\n",
    "            if line.startswith(\"-DOCSTART-\"): #let's not worry about these for the time being\n",
    "                continue\n",
    "            if not line: #sentence break\n",
    "                if current_sentence: #if we gathered a sentence, we should yield it, because a new one starts\n",
    "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
    "                    current_sentence=[] #...and start a new one\n",
    "                continue\n",
    "            #if we made it here, we are on a normal line\n",
    "            columns=line.split() #an actual word line\n",
    "            assert len(columns)==4 #we should have four columns, looking at the data\n",
    "            current_sentence.append(OneWord(*columns)) #* expands columns as arguments to OneWord constructor\n",
    "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
    "            if current_sentence: #yield also the last one!\n",
    "                yield current_sentence\n",
    "\n",
    "#Now just read the data in\n",
    "sentences_train=list(read_conll2003(\"dataset/train.txt\"))\n",
    "sentences_dev=list(read_conll2003(\"dataset/valid.txt\"))\n",
    "\n",
    "print(\"First three sentences\")\n",
    "for sent in sentences_dev[:3]:\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word_CRICKET': 1}, {'word_-': 1}, {'word_LEICESTERSHIRE': 1}, {'word_TAKE': 1}, {'word_OVER': 1}, {'word_AT': 1}, {'word_TOP': 1}, {'word_AFTER': 1}, {'word_INNINGS': 1}, {'word_VICTORY': 1}, {'word_.': 1}]\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence_features(sent):\n",
    "    #Given a sentence as a list of (word, label) pairs\n",
    "    #generate the features for every word\n",
    "    #The result should be a list of same length as the sentence\n",
    "    #Each item is a dictionary of {\"feature name\"->feature value} mappings, holding all features of the word at that position\n",
    "    \n",
    "    sent_features=[] #this will be the result\n",
    "    for one_word in sent:\n",
    "        #We must do nothing with label\n",
    "        #it just happens to be around\n",
    "        word_features={}\n",
    "        word_features[\"word_\"+one_word.word]=1 #the word itself is a feature\n",
    "        sent_features.append(word_features)\n",
    "    return sent_features\n",
    "\n",
    "print(generate_sentence_features(sentences_dev[0])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...now we can generate the training examples\n",
    "def prep_data(sentences):\n",
    "    all_labels=[] #here we gather labels for all words in all sentences\n",
    "    all_features=[] #here we gather features for all words in all sentences\n",
    "    for sentence in sentences:\n",
    "        sent_features=generate_sentence_features(sentence)\n",
    "        assert len(sent_features)==len(sentence)\n",
    "        #Now we can get, for every position its label and its features\n",
    "        for one_word,features in zip(sentence,sent_features):\n",
    "            all_labels.append(one_word.pos_label) #label\n",
    "            all_features.append(features)         #and features to go with it\n",
    "    return all_labels, all_features\n",
    "\n",
    "train_labels,train_features=prep_data(sentences_train)\n",
    "dev_labels,dev_features=prep_data(sentences_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer vocab size: 23623\n",
      "Train shape (203621, 23623)\n",
      "Dev shape (51362, 23623)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer=DictVectorizer()\n",
    "vectorizer.fit(train_features)\n",
    "print(\"Vectorizer vocab size:\",len(vectorizer.vocabulary_))\n",
    "\n",
    "feature_vectors_train=vectorizer.transform(train_features)\n",
    "feature_vectors_dev=vectorizer.transform(dev_features)\n",
    "\n",
    "print(\"Train shape\",feature_vectors_train.shape)\n",
    "print(\"Dev shape\",feature_vectors_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajays\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.05, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.05, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.05, verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "classifier=sklearn.svm.LinearSVC(C=0.05,verbose=1)\n",
    "classifier.fit(feature_vectors_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8655426190568903"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(feature_vectors_dev,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (203621, 68467)\n",
      "Dev shape (51362, 68467)\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajays\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9292862427475566"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sentence_features(sent):\n",
    "    #Given a sentence as a list of (word, label) pairs\n",
    "    #generate the features for every word\n",
    "    #The result should be a list of same length as the sentence\n",
    "    #Each item is a dictionary of {\"feature name\"->feature value} mappings, holding all features of the word at that position\n",
    "    \n",
    "    sent_features=[] #this will be the result\n",
    "    for word_idx, one_word in enumerate(sent):\n",
    "        #We do nothing with label\n",
    "        #it just happens to be around\n",
    "        word_features={}\n",
    "        word_features[\"word_\"+one_word.word]=1 #the word itself is a feature\n",
    "        if word_idx!=0:\n",
    "            word_features[\"left_word_\"+sent[word_idx-1].word]=1\n",
    "        if word_idx!=len(sent)-1:\n",
    "            word_features[\"right_word_\"+sent[word_idx+1].word]=1\n",
    "        sent_features.append(word_features)\n",
    "    return sent_features\n",
    "\n",
    "train_labels,train_features=prep_data(sentences_train)\n",
    "dev_labels,dev_features=prep_data(sentences_dev)\n",
    "vectorizer=DictVectorizer()\n",
    "vectorizer.fit(train_features)\n",
    "feature_vectors_train=vectorizer.transform(train_features)\n",
    "feature_vectors_dev=vectorizer.transform(dev_features)\n",
    "\n",
    "print(\"Train shape\",feature_vectors_train.shape)\n",
    "print(\"Dev shape\",feature_vectors_dev.shape)\n",
    "\n",
    "classifier=sklearn.svm.LinearSVC(C=1,verbose=1)\n",
    "classifier.fit(feature_vectors_train, train_labels)\n",
    "classifier.score(feature_vectors_dev,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP\n",
      "can MD\n",
      "house VB\n",
      "you PRP\n",
      "in IN\n",
      "my PRP$\n",
      "house NN\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# Let us try to look at some predictions\n",
    "sentence=\"I can house you in my house .\".split()\n",
    "\n",
    "sentence_data=[OneWord(w,\"XXX\",\"XXX\",\"XXX\") for w in sentence] #we need to fake this a bit, to get data in the correct format\n",
    "_,sentence_features=prep_data([sentence_data])\n",
    "sentence_vectors=vectorizer.transform(sentence_features)\n",
    "predictions=classifier.predict(sentence_vectors)\n",
    "for word,label in zip(sentence,predictions):\n",
    "    print(word,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned coefficients: (45, 68467)\n",
      "Classes in the data: ['\"' '$' \"''\" '(' ')' ',' '.' ':' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR'\n",
      " 'JJS' 'LS' 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'NN|SYM' 'PDT' 'POS' 'PRP' 'PRP$'\n",
      " 'RB' 'RBR' 'RBS' 'RP' 'SYM' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ'\n",
      " 'WDT' 'WP' 'WP$' 'WRB']\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned coefficients:\",classifier.coef_.shape)\n",
    "print(\"Classes in the data:\",classifier.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative features\n",
      "left_word_will\n",
      "left_word_Sale\n",
      "word_,\n",
      "left_word_going\n",
      "left_word_could\n",
      "left_word_would\n",
      "left_word_goals\n",
      "right_word_A-rated\n",
      "left_word_We\n",
      "word_and\n",
      "left_word_At\n",
      "word_in\n",
      "left_word_still\n",
      "right_word_announcement\n",
      "left_word_mixer\n",
      "left_word_can\n",
      "left_word_I\n",
      "left_word_should\n",
      "left_word_kms\n",
      "left_word_might\n",
      "left_word_8:00\n",
      "left_word_prices\n",
      "left_word_Mike\n",
      "right_word_SCOREBOARD\n",
      "left_word_must\n",
      "left_word_n't\n",
      "left_word_overs\n",
      "right_word_effect\n",
      "left_word_Services\n",
      "word_two\n",
      "-------------------------------\n",
      "Positive features\n",
      "word_world\n",
      "word_power\n",
      "word_consumer\n",
      "word_peace\n",
      "word_number\n",
      "word_hospital\n",
      "word_vouch\n",
      "word_cricket\n",
      "word_procure\n",
      "word_soccer\n",
      "word_victory\n",
      "word_championship\n",
      "word_staff\n",
      "word_motor\n",
      "word_value\n",
      "word_cabinet\n",
      "word_lunch\n",
      "word_rain\n",
      "word_injury\n",
      "word_league\n",
      "word_anyone\n",
      "word_UNION\n",
      "word_weekend\n",
      "word_edge\n",
      "word_parliament\n",
      "word_shutdown\n",
      "word_division\n",
      "word_cash\n",
      "word_tournament\n",
      "word_race\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "#Reverse the dictionary\n",
    "index2feature={}\n",
    "for feature,idx in vectorizer.vocabulary_.items():\n",
    "    assert idx not in index2feature #This really should hold\n",
    "    index2feature[idx]=feature\n",
    "#Now we can query index2feature to get the feature names as we need\n",
    "\n",
    "i=list(classifier.classes_).index(\"NN\") #which of the coefficients corresponds to nouns?\n",
    "indices=numpy.argsort(classifier.coef_[i])\n",
    "print(\"Negative features\")\n",
    "for idx in indices[:30]:\n",
    "    print(index2feature[idx])\n",
    "print(\"-------------------------------\")\n",
    "print(\"Positive features\")\n",
    "for idx in indices[::-1][:30]: #you can also do it the other way round, reverse, then pick\n",
    "    print(index2feature[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
